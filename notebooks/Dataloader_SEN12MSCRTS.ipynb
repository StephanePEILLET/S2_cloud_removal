{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c490478",
   "metadata": {},
   "source": [
    "# <center> Dataloader_SEN12MSCRTS </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d13b69",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecc18591",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import torch\n",
    "import h5py\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "from tqdm.auto import tqdm\n",
    "from natsort import natsorted\n",
    "from datetime import datetime\n",
    "to_date   = lambda string: datetime.strptime(string, '%Y-%m-%d')\n",
    "S1_LAUNCH = to_date('2014-04-03')\n",
    "\n",
    "# s2cloudless: see https://github.com/sentinel-hub/sentinel2-cloud-detector\n",
    "from s2cloudless import S2PixelCloudDetector\n",
    "\n",
    "import rasterio\n",
    "from rasterio.merge import merge\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from torch.utils.data import Dataset\n",
    "import scipy\n",
    "import scipy.signal as scisig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41a64c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fichier hdf5 sur la machine dl LNV87\n",
    "hdf5_file = Path(\"/media/hd/speillet/rpg_data/cloud_reconstruction/all.hdf5\")\n",
    "assert hdf5_file.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2736ee19-78e9-4ca6-beb6-c21b5aa8bbd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: <KeysViewHDF5 ['ROIs1970', 'ROIs2017']>\n",
      "Data keys: <KeysViewHDF5 ['21', '35', '40']>\n",
      "patch_data keys: <KeysViewHDF5 ['S1', 'S2', 'idx_cloudy_frames', 'idx_good_frames', 'idx_impaired_frames', 'valid_obs']>\n",
      "patch_data S1 <KeysViewHDF5 ['S1', 'S1_dates']>\n",
      "data S1 <HDF5 dataset \"S1\": shape (30, 2, 256, 256), type \"<f4\">\n",
      "dates S1 <HDF5 dataset \"S1_dates\": shape (30,), type \"|O\">\n",
      "S2 <KeysViewHDF5 ['S2', 'S2_dates', 'cloud_mask', 'cloud_prob']>\n",
      "data S2 <HDF5 dataset \"S2\": shape (30, 13, 256, 256), type \"<u2\">\n",
      "dates S2 <HDF5 dataset \"S2_dates\": shape (30,), type \"|O\">\n",
      "cloud_mask S2 <HDF5 dataset \"cloud_mask\": shape (30, 1, 256, 256), type \"|u1\">\n",
      "cloud_prob S2 <HDF5 dataset \"cloud_prob\": shape (30, 1, 256, 256), type \"<f4\">\n",
      "####################################################\n",
      "S2 <HDF5 dataset \"idx_cloudy_frames\": shape (13,), type \"<i8\">\n",
      "S2 <HDF5 dataset \"idx_good_frames\": shape (17,), type \"<i8\">\n",
      "S2 <HDF5 dataset \"idx_impaired_frames\": shape (13,), type \"<i8\">\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Dataset' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mS2\u001b[39m\u001b[38;5;124m\"\u001b[39m, patch_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124midx_good_frames\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mS2\u001b[39m\u001b[38;5;124m\"\u001b[39m, patch_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124midx_impaired_frames\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mS2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mpatch_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalid_obs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m())\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Dataset' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "with h5py.File(hdf5_file, \"r\") as f:\n",
    "    # Print all root level object names (aka keys) \n",
    "    # these can be group or dataset names \n",
    "    print(\"Keys: %s\" % f.keys())\n",
    "    roi = list(f.keys())[0]\n",
    "    zones = f[roi]\n",
    "    print(\"Data keys: %s\" % zones.keys())\n",
    "    patches = zones[list(zones)[0]]\n",
    "    # print(\"Elem keys: %s\" % patches.keys())\n",
    "    patch_data = patches[list(patches)[0]]\n",
    "    print(\"patch_data keys: %s\" % patch_data.keys())\n",
    "\n",
    "    print(\"patch_data S1\", patch_data['S1'].keys())\n",
    "    print(\"data S1\", patch_data['S1']['S1'])\n",
    "    print(\"dates S1\", patch_data['S1'][\"S1_dates\"])\n",
    "    print(\"S2\", patch_data['S2'].keys())\n",
    "    print(\"data S2\", patch_data['S2']['S2'])\n",
    "    print(\"dates S2\", patch_data['S2'][\"S2_dates\"])\n",
    "    print(\"cloud_mask S2\", patch_data['S2'][\"cloud_mask\"])\n",
    "    print(\"cloud_prob S2\", patch_data['S2'][\"cloud_prob\"])\n",
    "    # # preferred methods to get dataset values:\n",
    "    # ds_obj = f[a_group_key]      # returns as a h5py dataset object\n",
    "    # ds_arr = f[a_group_key][()]  # returns as a numpy array\n",
    "\n",
    "    print(\"####################################################\")\n",
    "    print(\"S2\", patch_data['idx_cloudy_frames'])\n",
    "    print(\"S2\", patch_data['idx_good_frames'])\n",
    "    print(\"S2\", patch_data['idx_impaired_frames'])\n",
    "    print(\"S2\", patch_data['valid_obs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43712e6-f31d-4e90-9ead-8754836d24c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "767fc91a",
   "metadata": {},
   "source": [
    "## UnCRtainTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c216a6c9",
   "metadata": {},
   "source": [
    "#### Utils functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ea9d5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_difference(channel1, channel2):\n",
    "    subchan = channel1 - channel2\n",
    "    sumchan = channel1 + channel2\n",
    "    sumchan[sumchan == 0] = 0.001  # checking for 0 divisions\n",
    "    return subchan / sumchan\n",
    "\n",
    "\n",
    "def get_shadow_mask(data_image):\n",
    "    data_image = data_image / 10000.\n",
    "\n",
    "    (ch, r, c) = data_image.shape\n",
    "    shadowmask = np.zeros((r, c)).astype('float32')\n",
    "\n",
    "    BB     = data_image[1]\n",
    "    BNIR   = data_image[7]\n",
    "    BSWIR1 = data_image[11]\n",
    "\n",
    "    CSI = (BNIR + BSWIR1) / 2.\n",
    "\n",
    "    t3 = 3/4 # cloud-score index threshold\n",
    "    T3 = np.min(CSI) + t3 * (np.mean(CSI) - np.min(CSI))\n",
    "\n",
    "    t4 = 5 / 6  # water-body index threshold\n",
    "    T4 = np.min(BB) + t4 * (np.mean(BB) - np.min(BB))\n",
    "\n",
    "    shadow_tf = np.logical_and(CSI < T3, BB < T4)\n",
    "\n",
    "    shadowmask[shadow_tf] = -1\n",
    "    shadowmask = scisig.medfilt2d(shadowmask, 5)\n",
    "\n",
    "    return shadowmask\n",
    "\n",
    "\n",
    "def get_cloud_mask(data_image, cloud_threshold, binarize=False, use_moist_check=False):\n",
    "    '''Adapted from https://github.com/samsammurphy/cloud-masking-sentinel2/blob/master/cloud-masking-sentinel2.ipynb'''\n",
    "\n",
    "    def _rescale(data, limits):\n",
    "        return (data - limits[0]) / (limits[1] - limits[0])\n",
    "    \n",
    "    data_image = data_image / 10000.\n",
    "    (ch, r, c) = data_image.shape\n",
    "\n",
    "    # Cloud until proven otherwise\n",
    "    score = np.ones((r, c)).astype('float32')\n",
    "    # Clouds are reasonably bright in the blue and aerosol/cirrus bands.\n",
    "    score = np.minimum(score, _rescale(data_image[1], [0.1, 0.5]))\n",
    "    score = np.minimum(score, _rescale(data_image[0], [0.1, 0.3]))\n",
    "    score = np.minimum(score, _rescale((data_image[0] + data_image[10]), [0.4, 0.9]))\n",
    "    score = np.minimum(score, _rescale((data_image[3] + data_image[2] + data_image[1]), [0.2, 0.8]))\n",
    "\n",
    "    if use_moist_check:\n",
    "        # Clouds are moist\n",
    "        ndmi = normalized_difference(data_image[7], data_image[11])\n",
    "        score = np.minimum(score, _rescale(ndmi, [-0.1, 0.1]))\n",
    "\n",
    "    # However, clouds are not snow.\n",
    "    ndsi = normalized_difference(data_image[2], data_image[11])\n",
    "    score = np.minimum(score, _rescale(ndsi, [0.8, 0.6]))\n",
    "\n",
    "    boxsize = 7\n",
    "    box = np.ones((boxsize, boxsize)) / (boxsize ** 2)\n",
    "\n",
    "    score = scipy.ndimage.morphology.grey_closing(score, size=(5, 5))\n",
    "    score = scisig.convolve2d(score, box, mode='same')\n",
    "\n",
    "    score = np.clip(score, 0.00001, 1.0)\n",
    "\n",
    "    if binarize:\n",
    "        score[score >= cloud_threshold] = 1\n",
    "        score[score < cloud_threshold]  = 0\n",
    "\n",
    "    return score\n",
    "\n",
    "# IN: [13 x H x W] S2 image (of arbitrary resolution H,W), scalar cloud detection threshold\n",
    "# OUT: cloud & shadow segmentation mask (of same resolution)\n",
    "# the multispectral S2 images are expected to have their default ranges and not be value-standardized yet\n",
    "# cloud_threshold: the higher the more conservative the masks (i.e. less pixels labeled clouds/shadows)\n",
    "def get_cloud_cloudshadow_mask(data_image, cloud_threshold):\n",
    "    cloud_mask = get_cloud_mask(data_image, cloud_threshold, binarize=True)\n",
    "    shadow_mask = get_shadow_mask(data_image)\n",
    "\n",
    "    # encode clouds and shadows as segmentation masks\n",
    "    cloud_cloudshadow_mask = np.zeros_like(cloud_mask)\n",
    "    cloud_cloudshadow_mask[shadow_mask < 0] = -1\n",
    "    cloud_cloudshadow_mask[cloud_mask > 0]  = 1\n",
    "\n",
    "    return cloud_cloudshadow_mask\n",
    "\n",
    "# utility functions used in the dataloaders of SEN12MS-CR and SEN12MS-CR-TS\n",
    "def read_tif(path_IMG):\n",
    "    tif = rasterio.open(path_IMG)\n",
    "    return tif\n",
    "\n",
    "def read_img(tif):\n",
    "    return tif.read().astype(np.float32)\n",
    "\n",
    "def rescale(img, oldMin, oldMax):\n",
    "    oldRange = oldMax - oldMin\n",
    "    img      = (img - oldMin) / oldRange\n",
    "    return img\n",
    "\n",
    "def process_MS(img, method):\n",
    "    if method=='default':\n",
    "        intensity_min, intensity_max = 0, 10000            # define a reasonable range of MS intensities\n",
    "        img = np.clip(img, intensity_min, intensity_max)   # intensity clipping to a global unified MS intensity range\n",
    "        img = rescale(img, intensity_min, intensity_max)   # project to [0,1], preserve global intensities (across patches), gets mapped to [-1,+1] in wrapper\n",
    "    if method=='resnet':\n",
    "        intensity_min, intensity_max = 0, 10000            # define a reasonable range of MS intensities\n",
    "        img = np.clip(img, intensity_min, intensity_max)   # intensity clipping to a global unified MS intensity range\n",
    "        img /= 2000                                        # project to [0,5], preserve global intensities (across patches)\n",
    "    img = np.nan_to_num(img)\n",
    "    return img\n",
    "\n",
    "def process_SAR(img, method):\n",
    "    if method=='default':\n",
    "        dB_min, dB_max = -25, 0                            # define a reasonable range of SAR dB\n",
    "        img = np.clip(img, dB_min, dB_max)                 # intensity clipping to a global unified SAR dB range\n",
    "        img = rescale(img, dB_min, dB_max)                 # project to [0,1], preserve global intensities (across patches), gets mapped to [-1,+1] in wrapper\n",
    "    if method=='resnet':\n",
    "        # project SAR to [0, 2] range\n",
    "        dB_min, dB_max = [-25.0, -32.5], [0, 0]\n",
    "        img = np.concatenate([(2 * (np.clip(img[0], dB_min[0], dB_max[0]) - dB_min[0]) / (dB_max[0] - dB_min[0]))[None, ...],\n",
    "                              (2 * (np.clip(img[1], dB_min[1], dB_max[1]) - dB_min[1]) / (dB_max[1] - dB_min[1]))[None, ...]], axis=0)\n",
    "    img = np.nan_to_num(img)\n",
    "    return img\n",
    "\n",
    "def get_cloud_cloudshadow_mask(img, cloud_threshold=0.2):\n",
    "    cloud_mask = get_cloud_mask(img, cloud_threshold, binarize=True)\n",
    "    shadow_mask = get_shadow_mask(img)\n",
    "\n",
    "    # encode clouds and shadows as segmentation masks\n",
    "    cloud_cloudshadow_mask = np.zeros_like(cloud_mask)\n",
    "    cloud_cloudshadow_mask[shadow_mask < 0] = -1\n",
    "    cloud_cloudshadow_mask[cloud_mask > 0] = 1\n",
    "\n",
    "    # label clouds and shadows\n",
    "    cloud_cloudshadow_mask[cloud_cloudshadow_mask != 0] = 1\n",
    "    return cloud_cloudshadow_mask\n",
    "\n",
    "\n",
    "# recursively apply function to nested dictionary\n",
    "def iterdict(dictionary, fct):\n",
    "    for k,v in dictionary.items():        \n",
    "        if isinstance(v, dict):\n",
    "            dictionary[k] = iterdict(v, fct)\n",
    "        else:      \n",
    "            dictionary[k] = fct(v)      \n",
    "    return dictionary\n",
    "\n",
    "def get_cloud_map(img, detector, instance=None):\n",
    "\n",
    "    # get cloud masks\n",
    "    img = np.clip(img, 0, 10000)\n",
    "    mask = np.ones((img.shape[-1], img.shape[-1]))\n",
    "    # note: if your model may suffer from dark pixel artifacts,\n",
    "    #       you may consider adjusting these filtering parameters\n",
    "    if not (img.mean()<1e-5 and img.std() <1e-5):\n",
    "        if detector == 'cloud_cloudshadow_mask':\n",
    "            threshold = 0.2  # set to e.g. 0.2 or 0.4\n",
    "            mask = get_cloud_cloudshadow_mask(img, threshold)\n",
    "        elif detector== 's2cloudless_map':\n",
    "            threshold = 0.5\n",
    "            mask = instance.get_cloud_probability_maps(np.moveaxis(img/10000, 0, -1)[None, ...])[0, ...]\n",
    "            mask[mask < threshold] = 0\n",
    "            mask = gaussian_filter(mask, sigma=2)\n",
    "        elif detector == 's2cloudless_mask':\n",
    "            mask = instance.get_cloud_masks(np.moveaxis(img/10000, 0, -1)[None, ...])[0, ...]\n",
    "        else:\n",
    "            mask = np.ones((img.shape[-1], img.shape[-1]))\n",
    "            warnings.warn(f'Method {detector} not yet implemented!')\n",
    "    else:   warnings.warn(f'Encountered a blank sample, defaulting to cloudy mask.')\n",
    "    return mask.astype(np.float32)\n",
    "\n",
    "\n",
    "# function to fetch paired data, which may differ in modalities or dates\n",
    "def get_pairedS1(patch_list, root_dir, mod=None, time=None):\n",
    "    paired_list = []\n",
    "    for patch in patch_list:\n",
    "        seed, roi, modality, time_number, fname = patch.split('/')\n",
    "        time = time_number if time is None else time # unless overwriting, ...\n",
    "        mod  = modality if mod is None else mod      # keep the patch list's original time and modality\n",
    "        n_patch       = fname.split('patch_')[-1].split('.tif')[0]\n",
    "        paired_dir    = os.path.join(seed, roi, mod.upper(), str(time))\n",
    "        candidates    = os.path.join(root_dir, paired_dir, f'{mod}_{seed}_{roi}_ImgNo_{time}_*_patch_{n_patch}.tif')\n",
    "        paired_list.append(os.path.join(paired_dir, os.path.basename(glob.glob(candidates)[0])))\n",
    "    return paired_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe96d64b",
   "metadata": {},
   "source": [
    "#### Dataset SEN12MSCRTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ae53235",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" SEN12MSCRTS data loader class, inherits from torch.utils.data.Dataset\n",
    "\n",
    "    IN: \n",
    "    root:               str, path to your copy of the SEN12MS-CR-TS data set\n",
    "    split:              str, in [all | train | val | test]\n",
    "    region:             str, [all | africa | america | asiaEast | asiaWest | europa]\n",
    "    cloud_masks:        str, type of cloud mask detector to run on optical data, in []\n",
    "    sample_type:        str, [generic | cloudy_cloudfree]\n",
    "    depricated --> vary_samples:       bool, whether to draw random samples across epochs or not, matters only if sample_type is 'cloud_cloudfree'\n",
    "    sampler             str, [fixed | fixedsubset | random]\n",
    "    n_input_samples:    int, number of input samples in time series\n",
    "    rescale_method:     str, [default | resnet]\n",
    "    min_cov:            float, in [0.0, 1.0]\n",
    "    max_cov:            float, in [0.0, 1.0]\n",
    "    import_data_path:   str, path to importing the suppl. file specifying what time points to load for input and output\n",
    "    \n",
    "    OUT:\n",
    "    data_loader:        SEN12MSCRTS instance, implements an iterator that can be traversed via __getitem__(pdx),\n",
    "                        which returns the pdx-th dictionary of patch-samples (whose structure depends on sample_type)\n",
    "\"\"\"\n",
    "\n",
    "class SEN12MSCRTS(Dataset):\n",
    "    def __init__(self, \n",
    "                 root, \n",
    "                 split=\"all\", \n",
    "                 region='all', \n",
    "                 cloud_masks='s2cloudless_mask', \n",
    "                 sample_type='cloudy_cloudfree', \n",
    "                 sampler='fixed', \n",
    "                 n_input_samples=3, \n",
    "                 rescale_method='default', \n",
    "                 min_cov=0.0, \n",
    "                 max_cov=1.0, \n",
    "                 import_data_path=None, \n",
    "                 custom_samples=None,\n",
    "                 ):\n",
    "        \n",
    "        self.root_dir = root   # set root directory which contains all ROI\n",
    "        self.region   = region # region according to which the ROI are selected\n",
    "        self.ROI      = {'ROIs1158': ['106'],\n",
    "                         'ROIs1868': ['17', '36', '56', '73', '85', '100', '114', '119', '121', '126', '127', '139', '142', '143'],\n",
    "                         'ROIs1970': ['20', '21', '35', '40', '57', '65', '71', '82', '83', '91', '112', '116', '119', '128', '132', '133', '135', '139', '142', '144', '149'],\n",
    "                         'ROIs2017': ['8', '22', '25', '32', '49', '61', '63', '69', '75', '103', '108', '115', '116', '117', '130', '140', '146']}\n",
    "        \n",
    "        # define splits conform with SEN12MS-CR\n",
    "        self.splits         = {}\n",
    "        if self.region=='all':\n",
    "            all_ROI             = [os.path.join(key, val) for key, vals in self.ROI.items() for val in vals]\n",
    "            self.splits['test'] = [os.path.join('ROIs1868', '119'), os.path.join('ROIs1970', '139'), os.path.join('ROIs2017', '108'), os.path.join('ROIs2017', '63'), os.path.join('ROIs1158', '106'), os.path.join('ROIs1868', '73'), os.path.join('ROIs2017', '32'),\n",
    "                                   os.path.join('ROIs1868', '100'), os.path.join('ROIs1970', '132'), os.path.join('ROIs2017', '103'), os.path.join('ROIs1868', '142'), os.path.join('ROIs1970', '20'), os.path.join('ROIs2017', '140')]  # official test split, across continents\n",
    "            self.splits['val']  = [os.path.join('ROIs2017', '22'), os.path.join('ROIs1970', '65'), os.path.join('ROIs2017', '117'), os.path.join('ROIs1868', '127'), os.path.join('ROIs1868', '17')] # insert a validation split here\n",
    "            self.splits['train']= [roi for roi in all_ROI if roi not in self.splits['val'] and roi not in self.splits['test']]  # all remaining ROI are used for training\n",
    "        elif self.region=='africa':\n",
    "            self.splits['test'] = [os.path.join('ROIs2017', '32'), os.path.join('ROIs2017', '140')]\n",
    "            self.splits['val']  = [os.path.join('ROIs2017', '22')]\n",
    "            self.splits['train']= [os.path.join('ROIs1970', '21'), os.path.join('ROIs1970', '35'), os.path.join('ROIs1970', '40'),\n",
    "                                   os.path.join('ROIs2017', '8'), os.path.join('ROIs2017', '61'), os.path.join('ROIs2017', '75')]\n",
    "        elif self.region=='america':\n",
    "            self.splits['test'] = [os.path.join('ROIs1158', '106'), os.path.join('ROIs1970', '132')]\n",
    "            self.splits['val']  = [os.path.join('ROIs1970', '65')]\n",
    "            self.splits['train']= [os.path.join('ROIs1868', '36'), os.path.join('ROIs1868', '85'),\n",
    "                                   os.path.join('ROIs1970', '82'), os.path.join('ROIs1970', '142'),\n",
    "                                   os.path.join('ROIs2017', '49'), os.path.join('ROIs2017', '116')]\n",
    "        elif self.region=='asiaEast':\n",
    "            self.splits['test'] = [os.path.join('ROIs1868', '73'), os.path.join('ROIs1868', '119'), os.path.join('ROIs1970', '139')]\n",
    "            self.splits['val']  = [os.path.join('ROIs2017', '117')]\n",
    "            self.splits['train']= [os.path.join('ROIs1868', '114'), os.path.join('ROIs1868', '126'), os.path.join('ROIs1868', '143'), \n",
    "                                   os.path.join('ROIs1970', '116'), os.path.join('ROIs1970', '135'),\n",
    "                                   os.path.join('ROIs2017', '25')]\n",
    "        elif self.region=='asiaWest':\n",
    "            self.splits['test'] = [os.path.join('ROIs1868', '100')]\n",
    "            self.splits['val']  = [os.path.join('ROIs1868', '127')]\n",
    "            self.splits['train']= [os.path.join('ROIs1970', '57'), os.path.join('ROIs1970', '83'), os.path.join('ROIs1970', '112'),\n",
    "                                   os.path.join('ROIs2017', '69'), os.path.join('ROIs2017', '115'), os.path.join('ROIs2017', '130')]\n",
    "        elif self.region=='europa':\n",
    "            self.splits['test'] = [os.path.join('ROIs2017', '63'), os.path.join('ROIs2017', '103'), os.path.join('ROIs2017', '108'), os.path.join('ROIs1868', '142'), os.path.join('ROIs1970', '20')]\n",
    "            self.splits['val']  = [os.path.join('ROIs1868', '17')]\n",
    "            self.splits['train']= [os.path.join('ROIs1868', '56'), os.path.join('ROIs1868', '121'), os.path.join('ROIs1868', '139'),\n",
    "                                   os.path.join('ROIs1970', '71'), os.path.join('ROIs1970', '91'), os.path.join('ROIs1970', '119'), os.path.join('ROIs1970', '128'), os.path.join('ROIs1970', '133'), os.path.join('ROIs1970', '144'), os.path.join('ROIs1970', '149'),\n",
    "                                   os.path.join('ROIs2017', '146')]\n",
    "        else: raise NotImplementedError\n",
    "\n",
    "        self.splits[\"all\"]  = self.splits[\"train\"] + self.splits[\"test\"] + self.splits[\"val\"]\n",
    "        self.split = split\n",
    "        \n",
    "        assert split in ['all', 'train', 'val', 'test'], \"Input dataset must be either assigned as all, train, test, or val!\"\n",
    "        assert sample_type in ['generic', 'cloudy_cloudfree'], \"Input data must be either generic or cloudy_cloudfree type!\"\n",
    "        assert cloud_masks in [None, 'cloud_cloudshadow_mask', 's2cloudless_map', 's2cloudless_mask'], \"Unknown cloud mask type!\"\n",
    "\n",
    "        self.modalities     = [\"S1\", \"S2\"]\n",
    "        self.time_points    = range(30)\n",
    "        self.cloud_masks    = cloud_masks  # e.g. 'cloud_cloudshadow_mask', 's2cloudless_map', 's2cloudless_mask'\n",
    "        self.sample_type    = sample_type if self.cloud_masks is not None else 'generic' # pick 'generic' or 'cloudy_cloudfree'\n",
    "        self.sampling       = sampler # type of sampler\n",
    "        self.vary_samples   = self.sampling =='random' if self.sample_type=='cloudy_cloudfree' else False # whether to draw different samples across epochs\n",
    "        self.n_input_t      = n_input_samples  # specifies the number of samples, if only part of the time series is used as an input\n",
    "\n",
    "        if self.vary_samples:\n",
    "            if self.split in ['val', 'test']:\n",
    "                warnings.warn(f'Loading {self.split} split, but sampled time points will differ each epoch!')\n",
    "            else:\n",
    "                warnings.warn(f'Randomly sampling targets, but remember to change seed if desiring different samples across models!')\n",
    "\n",
    "        if self.vary_samples:\n",
    "            self.t_windows = np.lib.stride_tricks.sliding_window_view(self.time_points, window_shape=self.n_input_t+1)\n",
    "\n",
    "        if self.cloud_masks in ['s2cloudless_map', 's2cloudless_mask']:\n",
    "            self.cloud_detector = S2PixelCloudDetector(threshold=0.4, all_bands=True, average_over=4, dilation_size=2)\n",
    "        else: self.cloud_detector = None\n",
    "\n",
    "        self.import_data_path = import_data_path\n",
    "        if self.import_data_path:\n",
    "            # fetch time points as specified in the imported file, expects arguments are set accordingly\n",
    "            if os.path.isdir(self.import_data_path):\n",
    "                import_here   = os.path.join(self.import_data_path, f'generic_{self.n_input_t}_{self.split}_{self.region}_{self.cloud_masks}.npy')\n",
    "            else:\n",
    "                import_here   = self.import_data_path\n",
    "            self.data_pairs   = np.load(import_here, allow_pickle=True).item()\n",
    "            self.n_data_pairs = len(self.data_pairs)\n",
    "            self.epoch_count  = 0 # count, for loading time points that vary across epochs\n",
    "            print(f'\\nImporting data pairings for split {self.split} from {import_here}.')\n",
    "        else: print('\\nData pairings are computed on the fly. Note. Pre-computing may speed up data loading')\n",
    "\n",
    "        self.custom_samples   = custom_samples\n",
    "        if isinstance (self.custom_samples, list):\n",
    "            self.paths            = self.custom_samples\n",
    "            self.import_data_path = None\n",
    "        else: self.paths            = self.get_paths()\n",
    "        self.n_samples        = len(self.paths)\n",
    "        # raise a warning that no data has been found\n",
    "        if not self.n_samples: self.throw_warn()\n",
    "\n",
    "        self.method          = rescale_method\n",
    "        self.min_cov, self.max_cov = min_cov, max_cov\n",
    "\n",
    "    def throw_warn(self):\n",
    "        warnings.warn(\"\"\"No data samples found! Please use the following directory structure:\n",
    "\n",
    "        path/to/your/SEN12MSCRTS/directory:\n",
    "        ├───ROIs1158\n",
    "        ├───ROIs1868\n",
    "        ├───ROIs1970\n",
    "        │   ├───20\n",
    "        │   ├───21\n",
    "        │   │   ├───S1\n",
    "        │   │   └───S2\n",
    "        │   │       ├───0\n",
    "        │   │       ├───1\n",
    "        │   │       │   └─── ... *.tif files\n",
    "        │   │       └───30\n",
    "        │   ...\n",
    "        └───ROIs2017\n",
    "\n",
    "        Note: the data is provided by ROI geo-spatially separated and sensor modalities individually.\n",
    "        You can simply merge the downloaded & extracted archives' subdirectories via 'mv */* .' in the parent directory\n",
    "        to obtain the required structure specified above, which the data loader expects.\n",
    "        \"\"\")\n",
    "\n",
    "    # indexes all patches contained in the current data split\n",
    "    def get_paths(self):  # assuming for the same ROI+num, the patch numbers are the same\n",
    "        print(f'\\nProcessing paths for {self.split} split of region {self.region}')\n",
    "\n",
    "        paths = []\n",
    "        for roi_dir, rois in self.ROI.items():\n",
    "            for roi in tqdm(rois):\n",
    "                roi_path = os.path.join(self.root_dir, roi_dir, roi)\n",
    "                # skip non-existent ROI or ROI not part of the current data split\n",
    "                if not os.path.isdir(roi_path) or os.path.join(roi_dir, roi) not in self.splits[self.split]: continue\n",
    "                path_s1_t, path_s2_t = [], [],\n",
    "                for tdx in self.time_points:\n",
    "                    # working with directory under time stamp tdx\n",
    "                    path_s1_complete = os.path.join(roi_path, self.modalities[0], str(tdx))\n",
    "                    path_s2_complete = os.path.join(roi_path, self.modalities[1], str(tdx))\n",
    "\n",
    "                    # same as complete paths, truncating root directory's path\n",
    "                    path_s1 = os.path.join(roi_dir, roi, self.modalities[0], str(tdx))\n",
    "                    path_s2 = os.path.join(roi_dir, roi, self.modalities[1], str(tdx))\n",
    "\n",
    "                    # get list of files which contains all the patches at time tdx\n",
    "                    s1_t = natsorted([os.path.join(path_s1, f) for f in os.listdir(path_s1_complete) if (os.path.isfile(os.path.join(path_s1_complete, f)) and \".tif\" in f)])\n",
    "                    s2_t = natsorted([os.path.join(path_s2, f) for f in os.listdir(path_s2_complete) if (os.path.isfile(os.path.join(path_s2_complete, f)) and \".tif\" in f)])\n",
    "\n",
    "                    # same number of patches\n",
    "                    assert len(s1_t) == len(s2_t)\n",
    "\n",
    "                    # sort via file names according to patch number and store\n",
    "                    path_s1_t.append(s1_t)\n",
    "                    path_s2_t.append(s2_t)\n",
    "\n",
    "                # for each patch of the ROI, collect its time points and make this one sample\n",
    "                for pdx in range(len(path_s1_t[0])):\n",
    "                    sample = {\"S1\": [path_s1_t[tdx][pdx] for tdx in self.time_points],\n",
    "                              \"S2\": [path_s2_t[tdx][pdx] for tdx in self.time_points]}\n",
    "                    paths.append(sample)\n",
    "\n",
    "        return paths\n",
    "\n",
    "\n",
    "    def fixed_sampler(self, coverage, clear_tresh = 1e-3):\n",
    "        # sample custom time points from the current patch space in the current split\n",
    "        # sort observation indices according to cloud coverage, ascendingly\n",
    "        coverage_idx = np.argsort(coverage)\n",
    "        cloudless_idx = coverage_idx[0] # take the (earliest) least cloudy sample\n",
    "        # take the first n_input_t samples with cloud coverage e.g. in [0.1, 0.5], ...\n",
    "        inputs_idx = [pdx for pdx, perc in enumerate(coverage) if perc >= self.min_cov and perc <= self.max_cov][:self.n_input_t]\n",
    "        if len(inputs_idx) < self.n_input_t:\n",
    "            # ... if not exists then take the first n_input_t samples (except target patch)\n",
    "            inputs_idx = [pdx for pdx in range(len(coverage)) if pdx!=cloudless_idx][:self.n_input_t]\n",
    "            coverage_match   = False # flag input samples that didn't meet the required cloud coverage\n",
    "        else: coverage_match = True  # assume the requested amount of cloud coverage is met\n",
    "        # check whether the target meets the requested amount of clearness\n",
    "        if coverage[cloudless_idx] > clear_tresh: coverage_match = False\n",
    "        return inputs_idx, cloudless_idx, coverage_match\n",
    "\n",
    "    def fixedsubset_sampler(self, coverage, earliest_idx=0, latext_idx=30, clear_tresh = 1e-3):\n",
    "        # apply the fixed sampler on only a subsequence of the input sequence\n",
    "        inputs_idx, cloudless_idx, coverage_match = self.fixed_sampler(self, coverage[earliest_idx:latext_idx], clear_tresh)\n",
    "        # shift sampled indices by the offset of the subsequence\n",
    "        inputs_idx, cloudless_idx = [idx + earliest_idx for idx in inputs_idx], cloudless_idx + earliest_idx\n",
    "        # if the sampled indices do not meet the criteria, then default to sampling over the full time series\n",
    "        if not coverage_match: inputs_idx, cloudless_idx, coverage_match = self.fixed_sampler(self, coverage, clear_tresh)\n",
    "        return inputs_idx, cloudless_idx, coverage_match\n",
    "\n",
    "    def random_sampler(self, coverage, clear_tresh = 1e-3):\n",
    "        # sample a random target time point below 0.1% coverage (i.e. coverage<1e-3), or at min coverage\n",
    "        is_clear = np.argwhere(np.array(coverage)<clear_tresh).flatten()\n",
    "        try: cloudless_idx = is_clear[np.random.randint(0, len(is_clear))]\n",
    "        except: cloudless_idx = np.array(coverage).argmin()\n",
    "        # around this target time point, pick self.n_input_t input time points\n",
    "        windows = [window for window in self.t_windows if cloudless_idx in window]\n",
    "        # we pick the window with cloudless_idx centered such that input samples are temporally adjacent,\n",
    "        # alternatively: pick a causal window (with cloudless_idx at the end) or randomly sample input dates\n",
    "        inputs_idx = [input_t for input_t in windows[len(windows)//2] if input_t!=cloudless_idx]\n",
    "        coverage_match = True # note: not checking whether any requested cloud coverage is met in this mode   \n",
    "        return inputs_idx, cloudless_idx, coverage_match\n",
    "\n",
    "    def sampler(self, s1, s2, masks, coverage, clear_tresh = 1e-3, earliest_idx=0, latext_idx=30):\n",
    "        if self.sampling=='random':\n",
    "            inputs_idx, cloudless_idx, coverage_match = self.random_sampler(coverage, clear_tresh)\n",
    "        elif self.sampling=='fixedsubset':\n",
    "            inputs_idx, cloudless_idx, coverage_match = self.fixedsubset_sampler(coverage, clear_tresh, earliest_idx=earliest_idx, latext_idx=latext_idx)\n",
    "        else: # default to fixed sampler\n",
    "            inputs_idx, cloudless_idx, coverage_match = self.fixed_sampler(coverage, clear_tresh)\n",
    "        \n",
    "        input_s1, input_s2, input_masks = np.array(s1)[inputs_idx], np.array(s2)[inputs_idx], np.array(masks)[inputs_idx]\n",
    "        target_s1, target_s2, target_mask = np.array(s1)[cloudless_idx], np.array(s2)[cloudless_idx], np.array(masks)[cloudless_idx]\n",
    "\n",
    "        data = {\"input\":  [input_s1, input_s2, input_masks, inputs_idx], \n",
    "                \"target\": [target_s1, target_s2, target_mask, cloudless_idx],\n",
    "                \"match\":  coverage_match}\n",
    "        return data\n",
    "\n",
    "\n",
    "    # load images at a given patch pdx for given time points tdx\n",
    "    def get_imgs(self, pdx, tdx=range(0,30)):\n",
    "        # load the images and infer the masks\n",
    "        s1_tif   = [read_tif(os.path.join(self.root_dir, img)) for img in np.array(self.paths[pdx]['S1'])[tdx]]\n",
    "        s2_tif   = [read_tif(os.path.join(self.root_dir, img)) for img in np.array(self.paths[pdx]['S2'])[tdx]]\n",
    "        coord    = [list(tif.bounds) for tif in s2_tif]\n",
    "        s1       = [process_SAR(read_img(img), self.method) for img in s1_tif]\n",
    "        s2       = [read_img(img) for img in s2_tif]  # note: pre-processing happens after cloud detection\n",
    "        masks    = None if not self.cloud_masks else [get_cloud_map(img, self.cloud_masks, self.cloud_detector) for img in s2]\n",
    "\n",
    "        # get statistics and additional meta information\n",
    "        coverage = [np.mean(mask) for mask in masks]\n",
    "        s1_dates = [to_date(img.split('/')[-1].split('_')[5]) for img in np.array(self.paths[pdx]['S1'])[tdx]]\n",
    "        s2_dates = [to_date(img.split('/')[-1].split('_')[5]) for img in np.array(self.paths[pdx]['S2'])[tdx]]\n",
    "        s1_td    = [(date-S1_LAUNCH).days for date in s1_dates]\n",
    "        s2_td    = [(date-S1_LAUNCH).days for date in s2_dates]\n",
    "\n",
    "        return s1_tif, s2_tif, coord, s1, s2, masks, coverage, s1_dates, s2_dates, s1_td, s2_td\n",
    "\n",
    "    # function to merge (a temporal list of spatial lists containing) raster patches into a single rasterized patch\n",
    "    def mosaic_patches(self, paths):\n",
    "        src_files_to_mosaic = []\n",
    "\n",
    "        for tp in paths:\n",
    "            tp_mosaic = []\n",
    "            for sp in tp: # collect patches in space to mosaic over\n",
    "                src = rasterio.open(os.path.join(self.root_dir, sp))\n",
    "                tp_mosaic.append(src)\n",
    "            mosaic, out_trans = merge(tp_mosaic)\n",
    "            src_files_to_mosaic.append(mosaic.astype(np.float32))\n",
    "        return src_files_to_mosaic #, mosaic_meta\n",
    "\n",
    "    def getsample(self, pdx):\n",
    "        return self.__getitem__(pdx)\n",
    "\n",
    "    def __getitem__(self, pdx):  # get the time series of one patch\n",
    "\n",
    "        # get all images of patch pdx for online selection of dates tdx\n",
    "        #s1_tif, s2_tif, coord, s1, s2, masks, coverage, s1_dates, s2_dates, s1_td, s2_td = self.get_imgs(pdx)\n",
    "\n",
    "        if self.sample_type == 'cloudy_cloudfree':\n",
    "            # this sample type allows for four manners of sampling data:\n",
    "            # a) by loading custom-defined samples, b.i) & b.ii) based on importing pre-computed statistics, and c) for full online computations\n",
    "            if self.custom_samples:\n",
    "                in_s1_td = [(to_date(tdx[0].split('/')[-1].split('_')[-3])-S1_LAUNCH).days for tdx in self.paths[pdx]['input']['S1']]\n",
    "                in_s2_td = [(to_date(tdx[0].split('/')[-1].split('_')[-3])-S1_LAUNCH).days for tdx in self.paths[pdx]['input']['S2']]\n",
    "                tg_s1_td, tg_s2_td = [], []\n",
    "                in_coord, tg_coord = [], []\n",
    "                coverage_match     = True\n",
    "\n",
    "                custom = iterdict(self.custom_samples[pdx], self.mosaic_patches)\n",
    "\n",
    "                input_s1    = np.array([process_SAR(img, self.method) for img in custom['input']['S1']]) # is of shape (T, C_S1, H, W)\n",
    "                input_s2    = [process_MS(img, self.method) for img in custom['input']['S2']] # is of shape (T, C_S2, H, W)\n",
    "                input_masks = [] if not self.cloud_masks else [get_cloud_map(img, self.cloud_masks, self.cloud_detector) for img in custom['input']['S2']]\n",
    "\n",
    "                target_s1   = process_SAR(custom['target']['S1'], self.method)[0]\n",
    "                target_s2   = [process_MS(custom['target']['S2'], self.method)[0]]\n",
    "                target_mask = [] if not self.cloud_masks else [get_cloud_map(img, self.cloud_masks, self.cloud_detector) for img in custom['input']['S2']]\n",
    "\n",
    "            elif self.import_data_path: \n",
    "                # compute epoch-sensitive index, wrap-around if exceeds imported dates\n",
    "                adj_pdx = (self.epoch_count*self.__len__() + pdx) % self.n_data_pairs \n",
    "\n",
    "                if 'input' in self.data_pairs and 'target' in self.data_pairs:\n",
    "                    # b.i) import pre-computed date indices:\n",
    "                    #   1. read pre-computed date indices\n",
    "                    #   2. only read images and compute masks of pre-computed dates tdx for patch pdx\n",
    "                    inputs_idx, cloudless_idx, coverage_match = self.data_pairs[adj_pdx]['input'], self.data_pairs[adj_pdx]['target'], True\n",
    "                else:\n",
    "                    # b.ii) import pre-computed cloud coverage:\n",
    "                    #   1. read pre-computed cloud coverage\n",
    "                    #   2. sample dates tdx online, given cloud coverage\n",
    "                    #   3. only read images and compute masks of pre-computed dates tdx for patch pdx\n",
    "                    coverage = [stats.item() for stats in self.data_pairs[adj_pdx]['coverage']]\n",
    "\n",
    "                    if self.sampling=='random':\n",
    "                        inputs_idx, cloudless_idx, coverage_match = self.random_sampler(coverage)\n",
    "                    elif self.sampling=='fixedsubset':\n",
    "                        inputs_idx, cloudless_idx, coverage_match = self.fixedsubset_sampler(coverage, earliest_idx=0, latext_idx=30)\n",
    "                    else: # default to fixed sampler\n",
    "                        inputs_idx, cloudless_idx, coverage_match = self.fixed_sampler(coverage)\n",
    "                    \n",
    "                    #if self.vary_samples: inputs_idx, cloudless_idx, coverage_match = self.random_sampler(coverage)\n",
    "                    #else: inputs_idx, cloudless_idx, coverage_match = self.fixed_sampler(coverage)\n",
    "\n",
    "                in_s1_tif, in_s2_tif, in_coord, in_s1, in_s2, in_masks, in_coverage, in_s1_dates, in_s2_dates, in_s1_td, in_s2_td = self.get_imgs(pdx, inputs_idx)\n",
    "                tg_s1_tif, tg_s2_tif, tg_coord, tg_s1, tg_s2, tg_masks, tg_coverage, tg_s1_dates, tg_s2_dates, tg_s1_td, tg_s2_td = self.get_imgs(pdx, [cloudless_idx])\n",
    "\n",
    "                target_s1, target_s2, target_mask = np.array(tg_s1)[0], np.array(tg_s2)[0], np.array(tg_masks)[0]\n",
    "                input_s1, input_s2, input_masks   = np.array(in_s1), np.array(in_s2), np.array(in_masks)\n",
    "\n",
    "                data_samples = {\"input\":  [input_s1, input_s2, input_masks, inputs_idx], \n",
    "                                \"target\": [target_s1, target_s2, target_mask, cloudless_idx],\n",
    "                                \"match\":  coverage_match}\n",
    "            else:\n",
    "                # c) infer date indices online:\n",
    "                #   1. read all images and compute every mask indiscriminately\n",
    "                #   2. post-hoc select the most optimal dates tdx for patch pdx\n",
    "                s1_tif, s2_tif, coord, s1, s2, masks, coverage, s1_dates, s2_dates, s1_td, s2_td = self.get_imgs(pdx)\n",
    "                data_samples =self.sampler(s1, s2, masks, coverage, clear_tresh = 1e-3)\n",
    "\n",
    "            if not self.custom_samples:\n",
    "                input_s1, input_s2, input_masks, inputs_idx         = data_samples['input']\n",
    "                target_s1, target_s2, target_mask, cloudless_idx    = data_samples['target']\n",
    "                coverage_match                                      = data_samples['match']\n",
    "                \n",
    "                # preprocess S2 data (after cloud masks have been computed)\n",
    "                input_s2    = [process_MS(img, self.method) for img in input_s2]\n",
    "                target_s2   = [process_MS(target_s2, self.method)]\n",
    "                \n",
    "                if not self.import_data_path:\n",
    "                    in_s1_td, in_s2_td = [s1_td[idx] for idx in inputs_idx], [s2_td[idx] for idx in inputs_idx]\n",
    "                    tg_s1_td, tg_s2_td = [s1_td[cloudless_idx]], [s2_td[cloudless_idx]]\n",
    "                    in_coord, tg_coord = [coord[idx] for idx in inputs_idx], [coord[cloudless_idx]]\n",
    "\n",
    "            sample = {'input': {'S1': list(input_s1),\n",
    "                                'S2': input_s2,\n",
    "                                'masks': list(input_masks),\n",
    "                                'coverage': [np.mean(mask) for mask in input_masks],\n",
    "                                'S1 TD': in_s1_td, #[s1_td[idx] for idx in inputs_idx],\n",
    "                                'S2 TD': in_s2_td, #[s2_td[idx] for idx in inputs_idx],\n",
    "                                'S1 path': [] if self.custom_samples else [os.path.join(self.root_dir, self.paths[pdx]['S1'][idx]) for idx in inputs_idx],\n",
    "                                'S2 path': [] if self.custom_samples else [os.path.join(self.root_dir, self.paths[pdx]['S2'][idx]) for idx in inputs_idx],\n",
    "                                'idx': [] if self.custom_samples else inputs_idx,\n",
    "                                'coord': in_coord, #[coord[idx] for idx in inputs_idx],\n",
    "                                },\n",
    "                    'target': {'S1': [target_s1],\n",
    "                                'S2': target_s2,\n",
    "                                'masks': [target_mask],\n",
    "                                'coverage': [np.mean(target_mask)],\n",
    "                                'S1 TD': tg_s1_td, #[s1_td[cloudless_idx]],\n",
    "                                'S2 TD': tg_s2_td, #[s2_td[cloudless_idx]],\n",
    "                                'S1 path': [] if self.custom_samples else [os.path.join(self.root_dir, self.paths[pdx]['S1'][cloudless_idx])],\n",
    "                                'S2 path': [] if self.custom_samples else [os.path.join(self.root_dir, self.paths[pdx]['S2'][cloudless_idx])],\n",
    "                                'idx': [] if self.custom_samples else cloudless_idx,\n",
    "                                'coord': tg_coord, #[coord[cloudless_idx]],\n",
    "                                },\n",
    "                    'coverage bin': coverage_match\n",
    "                    }\n",
    "        \n",
    "        elif self.sample_type == 'generic':\n",
    "            # did not implement custom sampling for options other than 'cloudy_cloudfree' yet\n",
    "            if self.custom_samples: raise NotImplementedError\n",
    "\n",
    "            s1_tif, s2_tif, coord, s1, s2, masks, coverage, s1_dates, s2_dates, s1_td, s2_td = self.get_imgs(pdx)\n",
    "\n",
    "            sample = {'S1': s1,\n",
    "                      'S2': [process_MS(img, self.method) for img in s2],\n",
    "                      'masks': masks,\n",
    "                      'coverage': coverage,\n",
    "                      'S1 TD': s1_td,\n",
    "                      'S2 TD': s2_td,\n",
    "                      'S1 path': [os.path.join(self.root_dir, self.paths[pdx]['S1'][idx]) for idx in self.time_points],\n",
    "                      'S2 path': [os.path.join(self.root_dir, self.paths[pdx]['S2'][idx]) for idx in self.time_points],\n",
    "                      'coord': coord\n",
    "                      }\n",
    "        return sample\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        # length of generated list\n",
    "        return self.n_samples\n",
    "\n",
    "    def incr_epoch_count(self):\n",
    "        # increment epoch count by 1\n",
    "        self.epoch_count += 1\n",
    "\n",
    "\n",
    "\"\"\" SEN12MSCR data loader class, inherits from torch.utils.data.Dataset\n",
    "\n",
    "    IN: \n",
    "    root:               str, path to your copy of the SEN12MS-CR-TS data set\n",
    "    split:              str, in [all | train | val | test]\n",
    "    region:             str, [all | africa | america | asiaEast | asiaWest | europa]\n",
    "    cloud_masks:        str, type of cloud mask detector to run on optical data, in []\n",
    "    sample_type:        str, [generic | cloudy_cloudfree]\n",
    "    n_input_samples:    int, number of input samples in time series\n",
    "    rescale_method:     str, [default | resnet]\n",
    "    \n",
    "    OUT:\n",
    "    data_loader:        SEN12MSCRTS instance, implements an iterator that can be traversed via __getitem__(pdx),\n",
    "                        which returns the pdx-th dictionary of patch-samples (whose structure depends on sample_type)\n",
    "\"\"\"\n",
    "\n",
    "class SEN12MSCR(Dataset):\n",
    "    def __init__(self, root, split=\"all\", region='all', cloud_masks='s2cloudless_mask', sample_type='pretrain', rescale_method='default'):\n",
    "\n",
    "        self.root_dir = root                                # set root directory which contains all ROI\n",
    "        self.region   = region                              # region according to which the ROI are selected \n",
    "        if self.region != 'all': raise NotImplementedError  # TODO: currently only supporting 'all'\n",
    "        self.ROI      = {'ROIs1158': ['106'],\n",
    "                         'ROIs1868': ['17', '36', '56', '73', '85', '100', '114', '119', '121', '126', '127', '139', '142', '143'],\n",
    "                         'ROIs1970': ['20', '21', '35', '40', '57', '65', '71', '82', '83', '91', '112', '116', '119', '128', '132', '133', '135', '139', '142', '144', '149'],\n",
    "                         'ROIs2017': ['8', '22', '25', '32', '49', '61', '63', '69', '75', '103', '108', '115', '116', '117', '130', '140', '146']}\n",
    "        \n",
    "        # define splits conform with SEN12MS-CR-TS\n",
    "        self.splits         = {}\n",
    "        self.splits['train']= ['ROIs1970_fall_s1/s1_3', 'ROIs1970_fall_s1/s1_22', 'ROIs1970_fall_s1/s1_148', 'ROIs1970_fall_s1/s1_107', 'ROIs1970_fall_s1/s1_1', 'ROIs1970_fall_s1/s1_114', \n",
    "                               'ROIs1970_fall_s1/s1_135', 'ROIs1970_fall_s1/s1_40', 'ROIs1970_fall_s1/s1_42', 'ROIs1970_fall_s1/s1_31', 'ROIs1970_fall_s1/s1_149', 'ROIs1970_fall_s1/s1_64', \n",
    "                               'ROIs1970_fall_s1/s1_28', 'ROIs1970_fall_s1/s1_144', 'ROIs1970_fall_s1/s1_57', 'ROIs1970_fall_s1/s1_35', 'ROIs1970_fall_s1/s1_133', 'ROIs1970_fall_s1/s1_30', \n",
    "                               'ROIs1970_fall_s1/s1_134', 'ROIs1970_fall_s1/s1_141', 'ROIs1970_fall_s1/s1_112', 'ROIs1970_fall_s1/s1_116', 'ROIs1970_fall_s1/s1_37', 'ROIs1970_fall_s1/s1_26', \n",
    "                               'ROIs1970_fall_s1/s1_77', 'ROIs1970_fall_s1/s1_100', 'ROIs1970_fall_s1/s1_83', 'ROIs1970_fall_s1/s1_71', 'ROIs1970_fall_s1/s1_93', 'ROIs1970_fall_s1/s1_119', \n",
    "                               'ROIs1970_fall_s1/s1_104', 'ROIs1970_fall_s1/s1_136', 'ROIs1970_fall_s1/s1_6', 'ROIs1970_fall_s1/s1_41', 'ROIs1970_fall_s1/s1_125', 'ROIs1970_fall_s1/s1_91', \n",
    "                               'ROIs1970_fall_s1/s1_131', 'ROIs1970_fall_s1/s1_120', 'ROIs1970_fall_s1/s1_110', 'ROIs1970_fall_s1/s1_19', 'ROIs1970_fall_s1/s1_14', 'ROIs1970_fall_s1/s1_81', \n",
    "                               'ROIs1970_fall_s1/s1_39', 'ROIs1970_fall_s1/s1_109', 'ROIs1970_fall_s1/s1_33', 'ROIs1970_fall_s1/s1_88', 'ROIs1970_fall_s1/s1_11', 'ROIs1970_fall_s1/s1_128', \n",
    "                               'ROIs1970_fall_s1/s1_142', 'ROIs1970_fall_s1/s1_122', 'ROIs1970_fall_s1/s1_4', 'ROIs1970_fall_s1/s1_27', 'ROIs1970_fall_s1/s1_147', 'ROIs1970_fall_s1/s1_85', \n",
    "                               'ROIs1970_fall_s1/s1_82', 'ROIs1970_fall_s1/s1_105', 'ROIs1158_spring_s1/s1_9', 'ROIs1158_spring_s1/s1_1', 'ROIs1158_spring_s1/s1_124', 'ROIs1158_spring_s1/s1_40', \n",
    "                               'ROIs1158_spring_s1/s1_101', 'ROIs1158_spring_s1/s1_21', 'ROIs1158_spring_s1/s1_134', 'ROIs1158_spring_s1/s1_145', 'ROIs1158_spring_s1/s1_141', 'ROIs1158_spring_s1/s1_66', \n",
    "                               'ROIs1158_spring_s1/s1_8', 'ROIs1158_spring_s1/s1_26', 'ROIs1158_spring_s1/s1_77', 'ROIs1158_spring_s1/s1_113', 'ROIs1158_spring_s1/s1_100', \n",
    "                               'ROIs1158_spring_s1/s1_117', 'ROIs1158_spring_s1/s1_119', 'ROIs1158_spring_s1/s1_6', 'ROIs1158_spring_s1/s1_58', 'ROIs1158_spring_s1/s1_120', 'ROIs1158_spring_s1/s1_110', \n",
    "                               'ROIs1158_spring_s1/s1_126', 'ROIs1158_spring_s1/s1_115', 'ROIs1158_spring_s1/s1_121', 'ROIs1158_spring_s1/s1_39', 'ROIs1158_spring_s1/s1_109', 'ROIs1158_spring_s1/s1_63', \n",
    "                               'ROIs1158_spring_s1/s1_75', 'ROIs1158_spring_s1/s1_132', 'ROIs1158_spring_s1/s1_128', 'ROIs1158_spring_s1/s1_142', 'ROIs1158_spring_s1/s1_15', 'ROIs1158_spring_s1/s1_45', \n",
    "                               'ROIs1158_spring_s1/s1_97', 'ROIs1158_spring_s1/s1_147', 'ROIs1868_summer_s1/s1_90', 'ROIs1868_summer_s1/s1_87', 'ROIs1868_summer_s1/s1_25', 'ROIs1868_summer_s1/s1_124', \n",
    "                               'ROIs1868_summer_s1/s1_114', 'ROIs1868_summer_s1/s1_135', 'ROIs1868_summer_s1/s1_40', 'ROIs1868_summer_s1/s1_101', 'ROIs1868_summer_s1/s1_42', \n",
    "                               'ROIs1868_summer_s1/s1_31', 'ROIs1868_summer_s1/s1_36', 'ROIs1868_summer_s1/s1_139', 'ROIs1868_summer_s1/s1_56', 'ROIs1868_summer_s1/s1_133', 'ROIs1868_summer_s1/s1_55', \n",
    "                               'ROIs1868_summer_s1/s1_43', 'ROIs1868_summer_s1/s1_113', 'ROIs1868_summer_s1/s1_76', 'ROIs1868_summer_s1/s1_123', 'ROIs1868_summer_s1/s1_143', \n",
    "                               'ROIs1868_summer_s1/s1_93', 'ROIs1868_summer_s1/s1_125', 'ROIs1868_summer_s1/s1_89', 'ROIs1868_summer_s1/s1_120', 'ROIs1868_summer_s1/s1_126', 'ROIs1868_summer_s1/s1_72', \n",
    "                               'ROIs1868_summer_s1/s1_115', 'ROIs1868_summer_s1/s1_121', 'ROIs1868_summer_s1/s1_146', 'ROIs1868_summer_s1/s1_140', 'ROIs1868_summer_s1/s1_95', \n",
    "                               'ROIs1868_summer_s1/s1_102', 'ROIs1868_summer_s1/s1_7', 'ROIs1868_summer_s1/s1_11', 'ROIs1868_summer_s1/s1_132', 'ROIs1868_summer_s1/s1_15', 'ROIs1868_summer_s1/s1_137', \n",
    "                               'ROIs1868_summer_s1/s1_4', 'ROIs1868_summer_s1/s1_27', 'ROIs1868_summer_s1/s1_147', 'ROIs1868_summer_s1/s1_86', 'ROIs1868_summer_s1/s1_47', 'ROIs2017_winter_s1/s1_68', \n",
    "                               'ROIs2017_winter_s1/s1_25', 'ROIs2017_winter_s1/s1_62', 'ROIs2017_winter_s1/s1_135', 'ROIs2017_winter_s1/s1_42', 'ROIs2017_winter_s1/s1_64', 'ROIs2017_winter_s1/s1_21', \n",
    "                               'ROIs2017_winter_s1/s1_55', 'ROIs2017_winter_s1/s1_112', 'ROIs2017_winter_s1/s1_116', 'ROIs2017_winter_s1/s1_8', 'ROIs2017_winter_s1/s1_59', 'ROIs2017_winter_s1/s1_49', \n",
    "                               'ROIs2017_winter_s1/s1_104',  'ROIs2017_winter_s1/s1_81', 'ROIs2017_winter_s1/s1_146', 'ROIs2017_winter_s1/s1_75', \n",
    "                               'ROIs2017_winter_s1/s1_94', 'ROIs2017_winter_s1/s1_102', 'ROIs2017_winter_s1/s1_61', 'ROIs2017_winter_s1/s1_47',\n",
    "                               'ROIs1868_summer_s1/s1_100', # note: this ROI is also used for testing in SEN12MS-CR-TS. If you wish to combine both datasets, please comment out this line\n",
    "                               ]\n",
    "        self.splits['val']  = ['ROIs2017_winter_s1/s1_22', 'ROIs1868_summer_s1/s1_19', 'ROIs1970_fall_s1/s1_65', 'ROIs1158_spring_s1/s1_17', 'ROIs2017_winter_s1/s1_107', \n",
    "                               'ROIs1868_summer_s1/s1_80', 'ROIs1868_summer_s1/s1_127', 'ROIs2017_winter_s1/s1_130', 'ROIs1868_summer_s1/s1_17', 'ROIs2017_winter_s1/s1_84'] \n",
    "        self.splits['test'] = ['ROIs1158_spring_s1/s1_106', 'ROIs1158_spring_s1/s1_123', 'ROIs1158_spring_s1/s1_140', 'ROIs1158_spring_s1/s1_31', 'ROIs1158_spring_s1/s1_44', \n",
    "                               'ROIs1868_summer_s1/s1_119', 'ROIs1868_summer_s1/s1_73', 'ROIs1970_fall_s1/s1_139', 'ROIs2017_winter_s1/s1_108', 'ROIs2017_winter_s1/s1_63']\n",
    "\n",
    "        self.splits[\"all\"]  = self.splits[\"train\"] + self.splits[\"test\"] + self.splits[\"val\"]\n",
    "        self.split = split\n",
    "        \n",
    "        assert split in ['all', 'train', 'val', 'test'], \"Input dataset must be either assigned as all, train, test, or val!\"\n",
    "        assert sample_type in ['pretrain'], \"Input data must be pretrain!\"\n",
    "        assert cloud_masks in [None, 'cloud_cloudshadow_mask', 's2cloudless_map', 's2cloudless_mask'], \"Unknown cloud mask type!\"\n",
    "\n",
    "        self.modalities     = [\"S1\", \"S2\"]\n",
    "        self.cloud_masks    = cloud_masks   # e.g. 'cloud_cloudshadow_mask', 's2cloudless_map', 's2cloudless_mask'\n",
    "        self.sample_type    = sample_type   # e.g. 'pretrain'\n",
    "\n",
    "        self.time_points    = range(1)\n",
    "        self.n_input_t      = 1             # specifies the number of samples, if only part of the time series is used as an input\n",
    "\n",
    "        if self.cloud_masks in ['s2cloudless_map', 's2cloudless_mask']:\n",
    "            self.cloud_detector = S2PixelCloudDetector(threshold=0.4, all_bands=True, average_over=4, dilation_size=2)\n",
    "        else: self.cloud_detector = None\n",
    "\n",
    "        self.paths          = self.get_paths()\n",
    "        self.n_samples      = len(self.paths)\n",
    "\n",
    "        # raise a warning if no data has been found\n",
    "        if not self.n_samples: self.throw_warn()\n",
    "\n",
    "        self.method         = rescale_method\n",
    "\n",
    "    # indexes all patches contained in the current data split\n",
    "    def get_paths(self):  # assuming for the same ROI+num, the patch numbers are the same\n",
    "        print(f'\\nProcessing paths for {self.split} split of region {self.region}')\n",
    "\n",
    "        paths = []\n",
    "        seeds_S1 = natsorted([s1dir for s1dir in os.listdir(self.root_dir) if \"_s1\" in s1dir])\n",
    "        for seed in tqdm(seeds_S1):\n",
    "            rois_S1 = natsorted(os.listdir(os.path.join(self.root_dir, seed)))\n",
    "            for roi in rois_S1:\n",
    "                roi_dir  = os.path.join(self.root_dir, seed, roi)\n",
    "                paths_S1        = natsorted([os.path.join(roi_dir, s1patch) for s1patch in os.listdir(roi_dir)])\n",
    "                paths_S2        = [patch.replace('/s1', '/s2').replace('_s1', '_s2') for patch in paths_S1]\n",
    "                paths_S2_cloudy = [patch.replace('/s1', '/s2_cloudy').replace('_s1', '_s2_cloudy') for patch in paths_S1]\n",
    "\n",
    "                for pdx, _ in enumerate(paths_S1):\n",
    "                    # omit patches that are potentially unpaired\n",
    "                    if not all([os.path.isfile(paths_S1[pdx]), os.path.isfile(paths_S2[pdx]), os.path.isfile(paths_S2_cloudy[pdx])]): continue\n",
    "                    # don't add patch if not belonging to the selected split\n",
    "                    if not any([split_roi in paths_S1[pdx] for split_roi in self.splits[self.split]]): continue\n",
    "                    sample = {\"S1\":         paths_S1[pdx],\n",
    "                              \"S2\":         paths_S2[pdx],\n",
    "                              \"S2_cloudy\":  paths_S2_cloudy[pdx]}\n",
    "                    paths.append(sample)\n",
    "        return paths\n",
    "\n",
    "    def __getitem__(self, pdx):  # get the triplet of patch with ID pdx\n",
    "        s1_tif          = read_tif(os.path.join(self.root_dir, self.paths[pdx]['S1']))\n",
    "        s2_tif          = read_tif(os.path.join(self.root_dir, self.paths[pdx]['S2']))\n",
    "        s2_cloudy_tif   = read_tif(os.path.join(self.root_dir, self.paths[pdx]['S2_cloudy']))\n",
    "        coord           = list(s2_tif.bounds)\n",
    "        s1              = process_SAR(read_img(s1_tif), self.method)\n",
    "        s2              = read_img(s2_tif)           # note: pre-processing happens after cloud detection\n",
    "        s2_cloudy       = read_img(s2_cloudy_tif)    # note: pre-processing happens after cloud detection\n",
    "        mask            = None if not self.cloud_masks else get_cloud_map(s2_cloudy, self.cloud_masks, self.cloud_detector)\n",
    "\n",
    "        sample = {'input': {'S1': s1,\n",
    "                            'S2': process_MS(s2_cloudy, self.method),\n",
    "                            'masks': mask,\n",
    "                            'coverage': np.mean(mask),\n",
    "                            'S1 path': os.path.join(self.root_dir, self.paths[pdx]['S1']),\n",
    "                            'S2 path': os.path.join(self.root_dir, self.paths[pdx]['S2_cloudy']),\n",
    "                            'coord': coord,\n",
    "                            },\n",
    "                    'target': {'S2': process_MS(s2, self.method),\n",
    "                               'S2 path': os.path.join(self.root_dir, self.paths[pdx]['S2']),\n",
    "                               'coord': coord,\n",
    "                                },\n",
    "                    }\n",
    "        return sample\n",
    "\n",
    "    def throw_warn(self):\n",
    "        warnings.warn(\"\"\"No data samples found! Please use the following directory structure:\n",
    "\n",
    "        path/to/your/SEN12MSCR/directory:\n",
    "            ├───ROIs1158_spring_s1\n",
    "            |   ├─s1_1\n",
    "            |   |   |...\n",
    "            |   |   ├─ROIs1158_spring_s1_1_p407.tif\n",
    "            |   |   |...\n",
    "            |    ...\n",
    "            ├───ROIs1158_spring_s2\n",
    "            |   ├─s2_1\n",
    "            |   |   |...\n",
    "            |   |   ├─ROIs1158_spring_s2_1_p407.tif\n",
    "            |   |   |...\n",
    "            |    ...\n",
    "            ├───ROIs1158_spring_s2_cloudy\n",
    "            |   ├─s2_cloudy_1\n",
    "            |   |   |...\n",
    "            |   |   ├─ROIs1158_spring_s2_cloudy_1_p407.tif\n",
    "            |   |   |...\n",
    "            |    ...\n",
    "            ...\n",
    "\n",
    "        Note: Please arrange the dataset in a format as e.g. provided by the script dl_data.sh.\n",
    "        \"\"\")\n",
    "\n",
    "    def __len__(self):\n",
    "        # length of generated list\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "525568e8-403d-44fc-9ade-852b7958c589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing paths for all split of region all\n"
     ]
    },
    {
     "ename": "NotADirectoryError",
     "evalue": "[Errno 20] Not a directory: '/media/hd/speillet/rpg_data/cloud_reconstruction/all.hdf5'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mSEN12MSCR\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mall\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 504\u001b[0m, in \u001b[0;36mSEN12MSCR.__init__\u001b[0;34m(self, root, split, region, cloud_masks, sample_type, rescale_method)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcloud_detector \u001b[38;5;241m=\u001b[39m S2PixelCloudDetector(threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.4\u001b[39m, all_bands\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, average_over\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, dilation_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcloud_detector \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 504\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpaths          \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_paths\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_samples      \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpaths)\n\u001b[1;32m    507\u001b[0m \u001b[38;5;66;03m# raise a warning if no data has been found\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 517\u001b[0m, in \u001b[0;36mSEN12MSCR.get_paths\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mProcessing paths for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m split of region \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    516\u001b[0m paths \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 517\u001b[0m seeds_S1 \u001b[38;5;241m=\u001b[39m natsorted([s1dir \u001b[38;5;28;01mfor\u001b[39;00m s1dir \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot_dir\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_s1\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m s1dir])\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m seed \u001b[38;5;129;01min\u001b[39;00m tqdm(seeds_S1):\n\u001b[1;32m    519\u001b[0m     rois_S1 \u001b[38;5;241m=\u001b[39m natsorted(os\u001b[38;5;241m.\u001b[39mlistdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_dir, seed)))\n",
      "\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: '/media/hd/speillet/rpg_data/cloud_reconstruction/all.hdf5'"
     ]
    }
   ],
   "source": [
    "ds = SEN12MSCR(\n",
    "    root=path_data,\n",
    "    region=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce301058",
   "metadata": {},
   "outputs": [],
   "source": [
    "root:               str, path to your copy of the SEN12MS-CR-TS data set\n",
    "split:              str, in [all | train | val | test]\n",
    "region:             str, [all | africa | america | asiaEast | asiaWest | europa]\n",
    "cloud_masks:        str, type of cloud mask detector to run on optical data, in []\n",
    "sample_type:        str, [generic | cloudy_cloudfree]\n",
    "n_input_samples:    int, number of input samples in time series\n",
    "rescale_method:     str, [default | resnet]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21502f5e-5b34-4011-a184-9e05f620d7ef",
   "metadata": {},
   "source": [
    "## U-TILISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "88a72df0-74ce-424c-b840-662481472940",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SEN12MSCRTSDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m hdf5_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall.hdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m dset \u001b[38;5;241m=\u001b[39m \u001b[43mSEN12MSCRTSDataset\u001b[49m(root\u001b[38;5;241m=\u001b[39mroot, hdf5_file\u001b[38;5;241m=\u001b[39mhdf5_file, split\u001b[38;5;241m=\u001b[39msplit)\n\u001b[1;32m      6\u001b[0m paths \u001b[38;5;241m=\u001b[39m dset\u001b[38;5;241m.\u001b[39mpaths\n\u001b[1;32m      7\u001b[0m dset\u001b[38;5;241m.\u001b[39mf\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SEN12MSCRTSDataset' is not defined"
     ]
    }
   ],
   "source": [
    "root=\"/media/hd/speillet/rpg_data/cloud_reconstruction/\"\n",
    "hdf5_file=\"all.hdf5\"\n",
    "split=\"train\"\n",
    "\n",
    "# dset = SEN12MSCRTSDataset(root=root, hdf5_file=hdf5_file, split=split)\n",
    "# paths = dset.paths\n",
    "# dset.f.close()\n",
    "# del dset\n",
    "\n",
    "# with h5py.File(\"/media/hd/speillet/rpg_data/cloud_reconstruction/all.hdf5\", 'a', libver='latest') as f:\n",
    "with h5py.File(hdf5_file, \"r\") as f:\n",
    "    # Iterate over all data samples in the given data split: detection of cloudy/foggy frames\n",
    "    for path in tqdm(paths):\n",
    "        patch = f[path]\n",
    "\n",
    "        # Load the entire S2 satellite image time series, T x C x H x W\n",
    "        images = torch.from_numpy(patch['S2/S2'][:].astype(np.float32))\n",
    "        T = images.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "478024d6-bfef-4779-9699-36445ba0c4ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "BlockingIOError",
     "evalue": "[Errno 11] Unable to synchronously open file (unable to lock file, errno = 11, error message = 'Resource temporarily unavailable')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBlockingIOError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mh5py\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/media/hd/speillet/rpg_data/cloud_reconstruction/all.hdf5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ma\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlibver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlatest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Iterate over all data samples in the given data split: detection of cloudy/foggy frames\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(f\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[0;32m~/mambaforge/envs/cloud_reconstruction/lib/python3.12/site-packages/h5py/_hl/files.py:564\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    555\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[1;32m    556\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[1;32m    557\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[1;32m    558\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[1;32m    559\u001b[0m                      meta_block_size\u001b[38;5;241m=\u001b[39mmeta_block_size,\n\u001b[1;32m    560\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    561\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[1;32m    562\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[1;32m    563\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[0;32m--> 564\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswmr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[0;32m~/mambaforge/envs/cloud_reconstruction/lib/python3.12/site-packages/h5py/_hl/files.py:250\u001b[0m, in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;66;03m# Open in append mode (read/write).\u001b[39;00m\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;66;03m# If that fails, create a new file only if it won't clobber an\u001b[39;00m\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;66;03m# existing one (ACC_EXCL)\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 250\u001b[0m         fid \u001b[38;5;241m=\u001b[39m \u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mACC_RDWR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;66;03m# Not all drivers raise FileNotFoundError (commented those that do not)\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m fapl\u001b[38;5;241m.\u001b[39mget_driver() \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m    253\u001b[0m         h5fd\u001b[38;5;241m.\u001b[39mSEC2,\n\u001b[1;32m    254\u001b[0m         h5fd\u001b[38;5;241m.\u001b[39mDIRECT \u001b[38;5;28;01mif\u001b[39;00m direct_vfd \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    262\u001b[0m         h5fd\u001b[38;5;241m.\u001b[39mROS3D \u001b[38;5;28;01mif\u001b[39;00m ros3 \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    263\u001b[0m     ) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5f.pyx:102\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mBlockingIOError\u001b[0m: [Errno 11] Unable to synchronously open file (unable to lock file, errno = 11, error message = 'Resource temporarily unavailable')"
     ]
    }
   ],
   "source": [
    "with h5py.File(\"/media/hd/speillet/rpg_data/cloud_reconstruction/all.hdf5\", 'a', libver='latest') as f:\n",
    "    # Iterate over all data samples in the given data split: detection of cloudy/foggy frames\n",
    "    print(f.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e886c446-553e-42ee-a852-5dd67184d134",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
